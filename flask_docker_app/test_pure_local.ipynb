{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how many GPUs we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 (NVIDIA A10G):\n",
      "  Total Memory: 21.99 GB\n",
      "  Used Memory: 0.00 GB\n",
      "  Available Memory: 21.99 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpu_memory_simple():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No available CUDA GPU\")\n",
    "        return\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # GB\n",
    "        allocated = torch.cuda.memory_allocated(i) / (1024 ** 3)  # GB\n",
    "        free = (torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_reserved(i)) / (1024 ** 3)  # GB\n",
    "        \n",
    "        print(f\"GPU {i} ({name}):\")\n",
    "        print(f\"  Total Memory: {total:.2f} GB\")\n",
    "        print(f\"  Used Memory: {allocated:.2f} GB\")\n",
    "        print(f\"  Available Memory: {free:.2f} GB\")\n",
    "\n",
    "check_gpu_memory_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Expert load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    " \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def my_plot(topk_ids, selected=None, new_id=None):\n",
    "    count = Counter(topk_ids.flatten().tolist())\n",
    "    sorted_counts = sorted(count.items(), key=lambda x: x[0])\n",
    "    elements = [k for k, v in sorted_counts]\n",
    "    values = [v for k, v in sorted_counts] \n",
    "    colors = [  'skyblue' for v in values]\n",
    "    if selected != None:\n",
    "        for s in selected:\n",
    "            colors[s] = 'green'\n",
    "    if new_id!= None:\n",
    "        for n in new_id:\n",
    "            colors[n] = 'red'\n",
    "    # 绘制柱状图\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(elements, values, color=colors)\n",
    "    plt.title('Frequency of Each Element', fontsize=14)\n",
    "    plt.xlabel('Element', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    for i, value in enumerate(values):\n",
    "        plt.text(elements[i], value + 0.1, str(value), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/vllm_test_field/myvllm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-18 13:42:17 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 13:42:18,365\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'world_size': 4, 'rank': 0}, {'world_size': 4, 'rank': 1}, {'world_size': 4, 'rank': 2}, {'world_size': 4, 'rank': 3}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from vllm.model_executor.layers.fused_moe.fused_moe import fused_experts_impl \n",
    "import transformers\n",
    "from transformers import Qwen2MoeConfig \n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 假设已经加载了 model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-MoE-A2.7B\"\n",
    "config = transformers.AutoConfig.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B\")\n",
    "\n",
    "\n",
    "WORLD_SIZE =  4\n",
    "TOKEN_NUM= 1000\n",
    "workers = [dict() for _ in range(WORLD_SIZE)]\n",
    "for i in range(0,len(workers)):\n",
    "    workers[i][\"world_size\"] = WORLD_SIZE\n",
    "    workers[i][\"rank\"] =   i\n",
    "print(workers)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize rank weight in the original track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 2816, 2048])\n",
      "torch.Size([15, 2048, 1408])\n",
      "torch.Size([15, 2816, 2048])\n",
      "torch.Size([15, 2048, 1408])\n",
      "torch.Size([15, 2816, 2048])\n",
      "torch.Size([15, 2048, 1408])\n",
      "torch.Size([15, 2816, 2048])\n",
      "torch.Size([15, 2048, 1408])\n"
     ]
    }
   ],
   "source": [
    "def init_rank_weight(world_size, config ):\n",
    "    expert_num = config.num_experts\n",
    "    avg_expert = expert_num // world_size\n",
    "    w13_weight = torch.nn.Parameter(torch.randn(\n",
    "            avg_expert,\n",
    "            2 * config.moe_intermediate_size,\n",
    "        config.hidden_size,\n",
    "        dtype=torch.bfloat16), requires_grad=False).to(\"cuda\")\n",
    "    print(w13_weight.shape)\n",
    "    w2_weight =torch.nn.Parameter(torch.randn(\n",
    "        avg_expert,\n",
    "        config.hidden_size,\n",
    "        config.moe_intermediate_size ,\n",
    "        dtype=torch.bfloat16),\n",
    "        requires_grad=False).to(\"cuda\")\n",
    "    print(w2_weight.shape) \n",
    "    return w13_weight, w2_weight\n",
    "\n",
    "for worker in workers:\n",
    "    worker[\"w1\"] , worker[\"w2\"] = init_rank_weight(WORLD_SIZE, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input for normal vllm track and expert-duplication containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk_ids.shape torch.Size([1000, 4]) torch.int32\n",
      "topk_weights.shape torch.Size([1000, 4]) torch.float32\n",
      "hidden_states.shape torch.Size([1000, 2048]) torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_input(token_num, config):\n",
    "    topk_ids = torch.randint(0, config.num_experts, (token_num, config.num_experts_per_tok), dtype=torch.int32).to(\"cuda\")\n",
    "    topk_weights = torch.randn(token_num, config.num_experts_per_tok, dtype=torch.float32).to(\"cuda\")\n",
    "    hidden_states = torch.randn( token_num, config.hidden_size, dtype=torch.bfloat16).to(\"cuda\") # \n",
    "    print(\"topk_ids.shape\", topk_ids.shape, topk_ids.dtype)\n",
    "    print(\"topk_weights.shape\", topk_weights.shape, topk_weights.dtype)\n",
    "    print(\"hidden_states.shape\", hidden_states.shape, hidden_states.dtype)\n",
    "    return {\"topk_ids\":topk_ids,\"topk_weights\":topk_weights, \"hidden_states\":hidden_states }\n",
    "\n",
    "inputs = create_input( TOKEN_NUM, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize expert map for boths path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32) torch.Size([60])\n",
      "tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  1,  2,\n",
      "         3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32) torch.Size([60])\n",
      "tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  1,  2,  3,  4,  5,\n",
      "         6,  7,  8,  9, 10, 11, 12, 13, 14, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32) torch.Size([60])\n",
      "tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,\n",
      "         9, 10, 11, 12, 13, 14], device='cuda:0', dtype=torch.int32) torch.Size([60])\n"
     ]
    }
   ],
   "source": [
    "def init_expert_map(rank, world_size, total_expert_num):\n",
    "    avg_expert_num = total_expert_num // world_size\n",
    "    real_local_expert_ids = list(range(rank * avg_expert_num, (rank + 1) * avg_expert_num))\n",
    "    expert_map = [-1] * total_expert_num\n",
    "    for local_id, global_id in enumerate(real_local_expert_ids):\n",
    "        expert_map[global_id] = local_id\n",
    "    return torch.tensor(expert_map, device=\"cuda\", dtype=torch.int32)\n",
    "\n",
    "for worker in workers:\n",
    "    worker[\"expert_map\"]  = init_expert_map( worker[\"rank\"],WORLD_SIZE,  config.num_experts)\n",
    "    print(worker[\"expert_map\"], worker[\"expert_map\"].shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-18 13:42:34 [fused_moe.py:954] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/ubuntu/vllm_test_field/vllm/vllm/model_executor/layers/fused_moe/configs/E=15,N=1408,device_name=NVIDIA_A10G.json\n",
      "INFO 05-18 13:42:34 [fused_moe.py:1658] expert_ids.shape: torch.Size([122])\n",
      "INFO 05-18 13:42:34 [fused_moe.py:1658] expert_ids.shape: torch.Size([122])\n",
      "INFO 05-18 13:42:34 [fused_moe.py:1658] expert_ids.shape: torch.Size([122])\n",
      "INFO 05-18 13:42:34 [fused_moe.py:1658] expert_ids.shape: torch.Size([122])\n"
     ]
    }
   ],
   "source": [
    "def moe_forward(worker, inputs ):\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    start_event.record()\n",
    "    output = fused_experts_impl(\n",
    "        hidden_states = inputs[\"hidden_states\"],\n",
    "        w1 = worker[\"w1\"],\n",
    "        w2 = worker[\"w2\"],\n",
    "        topk_weights = inputs[\"topk_weights\"],\n",
    "        topk_ids = inputs[\"topk_ids\"],\n",
    "        inplace = True,\n",
    "        activation = \"silu\",\n",
    "        expert_map = worker[\"expert_map\"],\n",
    "        global_num_experts =worker[\"expert_map\"].shape[0]\n",
    "    )\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "    latency_ms = start_event.elapsed_time(end_event)\n",
    "    return output,latency_ms\n",
    "\n",
    "make_inputs={\"topk_ids\": torch.randint(low=0, high=60, size=inputs[\"topk_ids\"].shape, dtype=inputs[\"topk_ids\"].dtype,device=inputs[\"topk_ids\"].device),\n",
    "             \"topk_weights\": torch.rand_like(inputs[\"topk_weights\"], dtype=inputs[\"topk_weights\"].dtype,device=inputs[\"topk_weights\"].device),\n",
    "             \"hidden_states\": torch.rand_like(inputs[\"hidden_states\"], dtype=inputs[\"hidden_states\"].dtype,device=inputs[\"hidden_states\"].device)}\n",
    "\n",
    "for worker in workers:\n",
    "    moe_forward (worker, make_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without expert duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2048])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-18 13:42:34 [fused_moe.py:1658] expert_ids.shape: torch.Size([122])\n",
      "torch.Size([1000, 2048])\n",
      "INFO 05-18 13:42:34 [fused_moe.py:1658] expert_ids.shape: torch.Size([122])\n",
      "torch.Size([1000, 2048])\n",
      "INFO 05-18 13:42:34 [fused_moe.py:1658] expert_ids.shape: torch.Size([122])\n",
      "torch.Size([1000, 2048])\n",
      "INFO 05-18 13:42:34 [fused_moe.py:1658] expert_ids.shape: torch.Size([122])\n",
      "torch.Size([1000, 2048])\n",
      "Rank 0 latency: 2.0346879959106445 ms\n",
      "Rank 1 latency: 1.5892479419708252 ms\n",
      "Rank 2 latency: 1.6076799631118774 ms\n",
      "Rank 3 latency: 1.58515202999115 ms\n",
      "End to End time is: 7.54 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time() \n",
    "without_expert_duplication_time_cost=[]\n",
    "for worker in workers:\n",
    "    worker[\"output\"],latency_ms = moe_forward (worker, inputs)\n",
    "    without_expert_duplication_time_cost.append(latency_ms)\n",
    "    print(worker[\"output\"].shape)\n",
    "\n",
    "for i in range(len(without_expert_duplication_time_cost)):\n",
    "    print(f\"Rank {i} latency: {without_expert_duplication_time_cost[i]} ms\")\n",
    "end_time = time.time()  # 记录结束时间\n",
    "elapsed_time = end_time - start_time  # 计算耗时\n",
    "\n",
    "\n",
    "print(f\"End to End time is: {elapsed_time*1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "print(\"finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
