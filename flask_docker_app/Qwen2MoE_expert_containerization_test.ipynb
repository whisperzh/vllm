{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51d0223",
   "metadata": {},
   "source": [
    "## Container Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017758fb",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef62967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import docker\n",
    "import requests\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "\n",
    "# client = docker.from_env()\n",
    "\n",
    "# --- Configs ---\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "NUM_CONTAINERS = 2\n",
    "IMAGE_NAME = \"moe_expert\"\n",
    "EXPERT_TIMEOUT = 20  # seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213789a",
   "metadata": {},
   "source": [
    "### Build Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0946db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (1/1)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 896B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/nvidia/cuda:12.1.1-base-ubuntu  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 896B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/nvidia/cuda:12.1.1-base-ubuntu  0.2s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (11/12)                                        docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 896B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/nvidia/cuda:12.1.1-base-ubuntu  0.2s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/7] FROM docker.io/nvidia/cuda:12.1.1-base-ubuntu22.04@sha256:457a4  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 821.81kB                                      0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/7] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/7] RUN apt-get update && apt-get install -y git python3 pyt  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/7] RUN pip3 install -v flask                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/7] RUN pip3 install -v vllm                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/7] COPY expert.py /app                                       0.0s\n",
      "\u001b[0m\u001b[34m => [7/7] COPY ./cuda_tools/libipc_tensor_tool.so /app                     0.0s\n",
      "\u001b[0m => exporting to image                                                     0.0s\n",
      "\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (12/12) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 896B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/nvidia/cuda:12.1.1-base-ubuntu  0.2s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/7] FROM docker.io/nvidia/cuda:12.1.1-base-ubuntu22.04@sha256:457a4  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 821.81kB                                      0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/7] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/7] RUN apt-get update && apt-get install -y git python3 pyt  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/7] RUN pip3 install -v flask                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/7] RUN pip3 install -v vllm                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/7] COPY expert.py /app                                       0.0s\n",
      "\u001b[0m\u001b[34m => [7/7] COPY ./cuda_tools/libipc_tensor_tool.so /app                     0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:1f78c9e71485321dc8ac7d84c4fe0a4f5f666a8678493  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/moe_expert                              0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! docker build -t moe_expert ~/vllm_test_field/vllm/flask_docker_app/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c80100",
   "metadata": {},
   "source": [
    "## Launching instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea405bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33d11858ae5d2899a68a829c25d74814b3e4b071afc316d07deeabb9e9f6f877\n",
      "fused_moe_layer_23_exp_0_29\n",
      "容器启动成功！\n",
      "7425fe02fa31ff10d285e53a8a399ab425be567b446295d55229fe412f170234\n",
      "fused_moe_layer_23_exp_30_59\n",
      "容器启动成功！\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# --- Step 2: Launch Expert Containers ---\n",
    "containers = []\n",
    "layer = 23\n",
    "\n",
    "experts_count_per_container = 60//NUM_CONTAINERS\n",
    "for i in range(NUM_CONTAINERS):\n",
    "    cmd = [\n",
    "    \"docker\", \"run\", \"-d\",\n",
    "    \"--name\", f\"fused_moe_layer_{layer}_exp_{i*experts_count_per_container}_{(i+1)*experts_count_per_container-1}\",\n",
    "    \"--gpus\", \"all\",\n",
    "    # \"--rm\",\n",
    "    \"-p\", f\"{5000+i}:5000\",\n",
    "    \"-v\", \"/home/ubuntu/vllm_test_field/vllm/ipc_handler_demo/weights:/app/weights\",\n",
    "    \"-e\", f\"RANK={i}\",\n",
    "    \"-e\", f\"NUM_EXPERTS={experts_count_per_container}\",\n",
    "    \"-e\", f\"GPU_IDX={0}\",\n",
    "    \"-e\", f\"WEIGHT_PATH=/app/weights\",\n",
    "    \"-e\", f\"LAYER={layer}\",\n",
    "    \n",
    "    IMAGE_NAME\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"fused_moe_layer_{layer}_exp_{i*experts_count_per_container}_{(i+1)*experts_count_per_container-1}\\n容器启动成功！\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"启动失败: {e}\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3eb9a9",
   "metadata": {},
   "source": [
    "### View if the instance is running correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24d3aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE        COMMAND                 CREATED         STATUS         PORTS                                         NAMES\n",
      "330ffb383d8e   moe_expert   \"python3 ./expert.py\"   3 seconds ago   Up 3 seconds   0.0.0.0:5001->5000/tcp, [::]:5001->5000/tcp   fused_moe_layer_23_exp_30_59\n",
      "ba41bfb0149a   moe_expert   \"python3 ./expert.py\"   4 seconds ago   Up 3 seconds   0.0.0.0:5000->5000/tcp, [::]:5000->5000/tcp   fused_moe_layer_23_exp_0_29\n"
     ]
    }
   ],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8acb0",
   "metadata": {},
   "source": [
    "## Testing container function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d359d29",
   "metadata": {},
   "source": [
    "### Load Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affb7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "def load_tensor(path):\n",
    "    tensors = {}\n",
    "    try:\n",
    "        tensors[\"hidden_states\"] = torch.load(path + \"hidden_states.pt\")\n",
    "        tensors[\"w1\"] = torch.load(path + \"w1.pt\")\n",
    "        tensors[\"w2\"] = torch.load(path + \"w2.pt\")\n",
    "        tensors[\"topk_weights\"] = torch.load(path + \"topk_weights.pt\")\n",
    "        tensors[\"topk_ids\"] = torch.load(path + \"topk_ids.pt\")\n",
    "        tensors[\"expert_map\"] = torch.load(path + \"expert_map.pt\")\n",
    "        tensors[\"out_hidden_states\"] = torch.load(path + \"out_hidden_states.pt\")\n",
    "        tensors[\"final_hidden_states\"] = torch.load(path + \"final_hidden_states.pt\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error: CUDA runtime issue - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    return tensors\n",
    "\n",
    "rank0 = load_tensor(\"/home/ubuntu/vllm_test_field/saved_tensors/rank_0/\")\n",
    "# rank1 = load_tensor(\"/home/ubuntu/vllm_test_field/saved_tensors/rank_1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce610d2",
   "metadata": {},
   "source": [
    "### Prepare handler and meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7fc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ctypes\n",
    "\n",
    "# Load the shared lib\n",
    "lib = ctypes.CDLL('/home/ubuntu/vllm_test_field/vllm/flask_docker_app/cuda_tools/libipc_tensor_tool.so')\n",
    "lib.export_ipc_handle.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n",
    "lib.export_ipc_handle.restype = ctypes.c_int\n",
    "\n",
    "def get_ipc_handle(tensor: torch.Tensor) -> bytes:\n",
    "    \n",
    "    meta={\n",
    "        \"shape\": tensor.shape,\n",
    "        \"dtype\": str(tensor.dtype),\n",
    "        \"device\": int(tensor.device.index),\n",
    "    }\n",
    "    if not tensor.is_cuda:\n",
    "        raise ValueError(\"Tensor must be on CUDA device\")\n",
    "\n",
    "    dev_ptr = tensor.data_ptr()\n",
    "    out = ctypes.create_string_buffer(64)\n",
    "\n",
    "    result = lib.export_ipc_handle(ctypes.c_void_p(dev_ptr), out)\n",
    "    if result != 0:\n",
    "        raise RuntimeError(f\"export_ipc_handle failed with code {result}\")\n",
    "\n",
    "    return out.raw, meta  # This is the 64-byte IPC handle\n",
    "\n",
    "hidden_states_handler, hidden_states_meta = get_ipc_handle(rank0[\"hidden_states\"])\n",
    "topk_weights_handler, topk_weights_meta = get_ipc_handle(rank0[\"topk_weights\"])\n",
    "topk_ids_handler, topk_ids_meta = get_ipc_handle(rank0[\"topk_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b21dd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'shape': torch.Size([6, 2048]), 'dtype': 'torch.bfloat16', 'device': 0},\n",
       " {'shape': torch.Size([6, 4]), 'dtype': 'torch.float32', 'device': 0},\n",
       " {'shape': torch.Size([6, 4]), 'dtype': 'torch.int32', 'device': 0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states_meta,topk_weights_meta,topk_ids_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834810fd",
   "metadata": {},
   "source": [
    "### Sending hidden states to instances for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c6d3a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_test_field/myvllm/lib/python3.12/site-packages/requests/models.py:974\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      5\u001b[39m response1 = requests.post(url1, \n\u001b[32m      6\u001b[39m                           data={\n\u001b[32m      7\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mhidden_states_meta\u001b[39m\u001b[33m'\u001b[39m: json.dumps(hidden_states_meta),\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtopk_ids_handler\u001b[39m\u001b[33m'\u001b[39m: (\u001b[33m'\u001b[39m\u001b[33mtopk_ids_handler.bin\u001b[39m\u001b[33m'\u001b[39m, topk_ids_handler, \u001b[33m'\u001b[39m\u001b[33mapplication/octet-stream\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     15\u001b[39m })\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# response2 = requests.post(url2, json={\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#         \"hidden_states\":rank1[\"hidden_states\"].cpu().tolist(),\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#         \"topk_weights\": rank1[\"topk_weights\"].cpu().tolist(),\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#         \"topk_ids\": rank1[\"topk_ids\"].cpu().tolist()\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#         })\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m output1 = torch.tensor(\u001b[43mresponse1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mhidden_output\u001b[39m\u001b[33m\"\u001b[39m],dtype=torch.bfloat16,device=\u001b[33m\"\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# output2 = torch.tensor(response2.json()[\"hidden_output\"],dtype=torch.bfloat16,device=\"cuda:0\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_test_field/myvllm/lib/python3.12/site-packages/requests/models.py:978\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "url1 = \"http://localhost:5000/forward\"\n",
    "# url2 = \"http://localhost:5001/forward\"\n",
    "\n",
    "response1 = requests.post(url1, \n",
    "                          data={\n",
    "        'hidden_states_meta': json.dumps(hidden_states_meta),\n",
    "        'topk_weights_meta': json.dumps(topk_weights_meta),\n",
    "        'topk_ids_meta': json.dumps(topk_ids_meta),\n",
    "},\n",
    "files={\n",
    "        'hidden_states_handler': ('hidden_states_handler.bin', hidden_states_handler, 'application/octet-stream'),\n",
    "        'topk_weights_handler': ('topk_weights_handler.bin', topk_weights_handler, 'application/octet-stream'),\n",
    "        'topk_ids_handler': ('topk_ids_handler.bin', topk_ids_handler, 'application/octet-stream'),\n",
    "})\n",
    "\n",
    "# response2 = requests.post(url2, json={\n",
    "#         \"hidden_states\":rank1[\"hidden_states\"].cpu().tolist(),\n",
    "#         \"topk_weights\": rank1[\"topk_weights\"].cpu().tolist(),\n",
    "#         \"topk_ids\": rank1[\"topk_ids\"].cpu().tolist()\n",
    "#         })\n",
    "\n",
    "output1 = torch.tensor(response1.json()[\"hidden_output\"],dtype=torch.bfloat16,device=\"cuda:0\")\n",
    "# output2 = torch.tensor(response2.json()[\"hidden_output\"],dtype=torch.bfloat16,device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1203b270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print( torch.equal(output1 , rank0[\"out_hidden_states\"]) )\n",
    "print( torch.equal(output2 , rank1[\"out_hidden_states\"].cuda()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa310b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "rank0[\"shared_output\"] = torch.load(\"/home/ubuntu/vllm_test_field/saved_tensors/rank_0/shared_output.pt\")\n",
    "rank1[\"shared_output\"] = torch.load(\"/home/ubuntu/vllm_test_field/saved_tensors/rank_1/shared_output.pt\")\n",
    "rank0_final_hidden_states = output1 + rank0[\"shared_output\"]\n",
    "rank1_final_hidden_states = output2 + rank1[\"shared_output\"].cuda()\n",
    "# all reduce \n",
    "reduced_result = rank0_final_hidden_states + rank1_final_hidden_states.to(rank0_final_hidden_states.device)\n",
    "print( torch.equal(reduced_result, \n",
    "                   rank0[\"final_hidden_states\"]) )\n",
    "\n",
    "print( torch.equal(reduced_result, \n",
    "                   rank1[\"final_hidden_states\"].cuda()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89fb7f",
   "metadata": {},
   "source": [
    "## Turn off the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342bab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76350462c9e7\n",
      "09770c2cbc63\n",
      "已停止 2 个容器: ['76350462c9e7', '09770c2cbc63']\n"
     ]
    }
   ],
   "source": [
    "def stop_fused_moe_containers():\n",
    "    try:\n",
    "        # 1. 获取所有名称包含 'fused_moe_layer' 的容器ID\n",
    "        grep_cmd = \"docker ps --filter 'name=fused_moe_layer' -q\"\n",
    "        container_ids = subprocess.check_output(grep_cmd, shell=True, text=True).strip().split('\\n')\n",
    "        \n",
    "        # 2. 批量停止容器\n",
    "        if container_ids and container_ids[0]:  # 如果有匹配的容器\n",
    "            stop_cmd = f\"docker stop {' '.join(container_ids)}\"\n",
    "            subprocess.run(stop_cmd, shell=True, check=True)\n",
    "            print(f\"已停止 {len(container_ids)} 个容器: {container_ids}\")\n",
    "        else:\n",
    "            print(\"没有找到名称包含 'fused_moe_layer' 的容器\")\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"操作失败: {e}\")\n",
    "\n",
    "# 执行函数\n",
    "stop_fused_moe_containers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
