{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf90370",
   "metadata": {},
   "source": [
    "## See how many GPUs we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c1da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 (NVIDIA A10G):\n",
      "  Total Memory: 21.99 GB\n",
      "  Used Memory: 0.00 GB\n",
      "  Available Memory: 21.99 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpu_memory_simple():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No available CUDA GPU\")\n",
    "        return\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # GB\n",
    "        allocated = torch.cuda.memory_allocated(i) / (1024 ** 3)  # GB\n",
    "        free = (torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_reserved(i)) / (1024 ** 3)  # GB\n",
    "        \n",
    "        print(f\"GPU {i} ({name}):\")\n",
    "        print(f\"  Total Memory: {total:.2f} GB\")\n",
    "        print(f\"  Used Memory: {allocated:.2f} GB\")\n",
    "        print(f\"  Available Memory: {free:.2f} GB\")\n",
    "\n",
    "check_gpu_memory_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75096103",
   "metadata": {},
   "source": [
    "## Plot Expert load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b188e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    " \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def my_plot(topk_ids, selected=None, new_id=None):\n",
    "    count = Counter(topk_ids.flatten().tolist())\n",
    "    sorted_counts = sorted(count.items(), key=lambda x: x[0])\n",
    "    elements = [k for k, v in sorted_counts]\n",
    "    values = [v for k, v in sorted_counts] \n",
    "    colors = [  'skyblue' for v in values]\n",
    "    if selected != None:\n",
    "        for s in selected:\n",
    "            colors[s] = 'green'\n",
    "    if new_id!= None:\n",
    "        for n in new_id:\n",
    "            colors[n] = 'red'\n",
    "    # 绘制柱状图\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(elements, values, color=colors)\n",
    "    plt.title('Frequency of Each Element', fontsize=14)\n",
    "    plt.xlabel('Element', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    for i, value in enumerate(values):\n",
    "        plt.text(elements[i], value + 0.1, str(value), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c800b",
   "metadata": {},
   "source": [
    "## Create Docker Image for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2593b9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      " => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 896B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/nvidia/cuda:12.1.1-base-ubuntu  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 896B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/nvidia/cuda:12.1.1-base-ubuntu  0.2s\n",
      "\u001b[0m\u001b[?25h"
      " => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 896B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/nvidia/cuda:12.1.1-base-ubuntu  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 896B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/nvidia/cuda:12.1.1-base-ubuntu  0.2s\n",
      "\u001b[0m\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (12/12) FINISHED                               docker:default\n",
      "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (12/12) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 896B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/nvidia/cuda:12.1.1-base-ubuntu  0.2s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/nvidia/cuda:12.1.1-base-ubuntu  0.2s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/7] FROM docker.io/nvidia/cuda:12.1.1-base-ubuntu22.04@sha256:457a4  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 114B                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/7] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/7] RUN apt-get update && apt-get install -y git python3 pyt  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/7] RUN pip3 install -v flask                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/7] RUN pip3 install -v vllm                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/7] COPY expert.py /app                                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [7/7] COPY ./cuda_tools/libipc_tensor_tool.so /app              0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:f86dba3720cddc8a8d5149577d12bd648e132eb25c972  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/moe_expert                              0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! docker build -t moe_expert ~/vllm_test_field/vllm/flask_docker_app/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be124d",
   "metadata": {},
   "source": [
    "# Serverless Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69790501",
   "metadata": {},
   "source": [
    "## Initialize workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c01cb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/vllm_test_field/myvllm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 09:22:39 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 09:22:41,892\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'world_size': 4, 'rank': 0}, {'world_size': 4, 'rank': 1}, {'world_size': 4, 'rank': 2}, {'world_size': 4, 'rank': 3}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from vllm.model_executor.layers.fused_moe.fused_moe import fused_experts_impl \n",
    "import transformers\n",
    "from transformers import Qwen2MoeConfig \n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 假设已经加载了 model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-MoE-A2.7B\"\n",
    "config = transformers.AutoConfig.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B\")\n",
    "\n",
    "\n",
    "WORLD_SIZE =  4\n",
    "TOKEN_NUM= 10000\n",
    "workers = [dict() for _ in range(WORLD_SIZE)]\n",
    "for i in range(0,len(workers)):\n",
    "    workers[i][\"world_size\"] = WORLD_SIZE\n",
    "    workers[i][\"rank\"] =   i\n",
    "print(workers)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a815a142",
   "metadata": {},
   "source": [
    "## Prepare input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ed9f330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk_ids.shape torch.Size([10000, 4]) torch.int32\n",
      "topk_weights.shape torch.Size([10000, 4]) torch.float32\n",
      "hidden_states.shape torch.Size([10000, 2048]) torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TOKEN_NUM= 10000\n",
    "def create_input(token_num, config):\n",
    "    topk_ids = torch.randint(0, config.num_experts, (token_num, config.num_experts_per_tok), dtype=torch.int32).to(\"cuda\")\n",
    "    topk_weights = torch.randn(token_num, config.num_experts_per_tok, dtype=torch.float32).to(\"cuda\")\n",
    "    hidden_states = torch.randn( token_num, config.hidden_size, dtype=torch.bfloat16).to(\"cuda\") # \n",
    "    print(\"topk_ids.shape\", topk_ids.shape, topk_ids.dtype)\n",
    "    print(\"topk_weights.shape\", topk_weights.shape, topk_weights.dtype)\n",
    "    print(\"hidden_states.shape\", hidden_states.shape, hidden_states.dtype)\n",
    "    return {\"topk_ids\":topk_ids,\"topk_weights\":topk_weights, \"hidden_states\":hidden_states }\n",
    "\n",
    "inputs = create_input( TOKEN_NUM, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d0294",
   "metadata": {},
   "source": [
    "## Launch Containers and warmup"
    "## Launch Containers and warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "execution_count": 6,
   "id": "8e324ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6bd4fbeaf968af3e8733e07e5c75a58d36500199ce9630f3350791364d2215d4\n"
      "6bd4fbeaf968af3e8733e07e5c75a58d36500199ce9630f3350791364d2215d4\n"
     ]
    },
    {
     "name": "stdout",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fused_moe_layer_23_exp_0_14\n",
      "容器启动成功！\n",
      "1b7265651933aac992ef9a07e4eb18f0ed66feb232217d9c3ed6fa3112ef0583\n",
      "fused_moe_layer_23_exp_60_74\n",
      "容器启动成功！\n",
      "13dd7c50281142ce1d33a46be6d7fc1d64278c236e5bfb52cdb36a9f852bdaef\n",
      "fused_moe_layer_23_exp_15_29\n",
      "容器启动成功！\n",
      "f265a7e89cbaafa9b4e5e3e87bc52777ae9a187bb06b7b4302d0f0758d231577\n",
      "fused_moe_layer_23_exp_75_89\n",
      "容器启动成功！\n",
      "a204251e1c4f3dcfce7a5494fd9d0f44a44b49c67c2896f44c36f4ea199a119e\n",
      "fused_moe_layer_23_exp_30_44\n",
      "容器启动成功！\n",
      "e56541c5d535933f07a7fdc52712bbbad9430546e0f4d3830dbd8f5964dfa349\n",
      "fused_moe_layer_23_exp_90_104\n",
      "容器启动成功！\n",
      "2706e6706f3e31819de29fb8c6c9d91d72b9cd0d79877abc18e581016287290c\n",
      "fused_moe_layer_23_exp_45_59\n",
      "容器启动成功！\n",
      "ae5362ebf0811ef702098cb98ea2b67335f0e0248222f43b335acb371d6621db\n",
      "fused_moe_layer_23_exp_105_119\n",
      "容器启动成功！\n",
      "Containers launched in: 6981.89 ms\n",
      "Average containers launching time : 1745.47 ms\n"
      "fused_moe_layer_23_exp_0_14\n",
      "容器启动成功！\n",
      "1b7265651933aac992ef9a07e4eb18f0ed66feb232217d9c3ed6fa3112ef0583\n",
      "fused_moe_layer_23_exp_60_74\n",
      "容器启动成功！\n",
      "13dd7c50281142ce1d33a46be6d7fc1d64278c236e5bfb52cdb36a9f852bdaef\n",
      "fused_moe_layer_23_exp_15_29\n",
      "容器启动成功！\n",
      "f265a7e89cbaafa9b4e5e3e87bc52777ae9a187bb06b7b4302d0f0758d231577\n",
      "fused_moe_layer_23_exp_75_89\n",
      "容器启动成功！\n",
      "a204251e1c4f3dcfce7a5494fd9d0f44a44b49c67c2896f44c36f4ea199a119e\n",
      "fused_moe_layer_23_exp_30_44\n",
      "容器启动成功！\n",
      "e56541c5d535933f07a7fdc52712bbbad9430546e0f4d3830dbd8f5964dfa349\n",
      "fused_moe_layer_23_exp_90_104\n",
      "容器启动成功！\n",
      "2706e6706f3e31819de29fb8c6c9d91d72b9cd0d79877abc18e581016287290c\n",
      "fused_moe_layer_23_exp_45_59\n",
      "容器启动成功！\n",
      "ae5362ebf0811ef702098cb98ea2b67335f0e0248222f43b335acb371d6621db\n",
      "fused_moe_layer_23_exp_105_119\n",
      "容器启动成功！\n",
      "Containers launched in: 6981.89 ms\n",
      "Average containers launching time : 1745.47 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import subprocess\n",
    "import time\n",
    "import torch\n",
    "# configs\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "NUM_CONTAINERS = WORLD_SIZE\n",
    "IMAGE_NAME = \"moe_expert\"\n",
    "BASE_PORT = 5000\n",
    "layer = 23\n",
    "experts_count_per_container = 60//NUM_CONTAINERS\n",
    "\n",
    "docker_start_time = time.time() \n",
    "\n",
    "for i in range(NUM_CONTAINERS):\n",
    "    cmd = [\n",
    "    \"docker\", \"run\", \"-d\",\n",
    "    \"--name\", f\"fused_moe_layer_{layer}_exp_{i*experts_count_per_container}_{(i+1)*experts_count_per_container-1}\",\n",
    "    \"--gpus\", \"all\",\n",
    "    \"--rm\",\n",
    "    \"--rm\",\n",
    "    \"-p\", f\"{BASE_PORT+i}:5000\",\n",
    "    \"-v\", \"/home/ubuntu/vllm_test_field/vllm/ipc_handler_demo/weights:/app/weights\",\n",
    "    \"-e\", f\"RANK={i}\",\n",
    "    \"-e\", f\"NUM_EXPERTS={experts_count_per_container}\",\n",
    "    \"-e\", f\"GPU_IDX={0}\",\n",
    "    \"-e\", f\"WEIGHT_PATH=/app/weights\",\n",
    "    \"-e\", f\"LAYER={layer}\",\n",
    "    \"-e\", f\"SELF_WARMUP={True}\",\n",
    "    \n",
    "    \n",
    "    IMAGE_NAME\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"fused_moe_layer_{layer}_exp_{i*experts_count_per_container}_{(i+1)*experts_count_per_container-1}\\n容器启动成功！\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"启动失败: {e}\")\n",
    "        \n",
    "    # duplicated container    \n",
    "    cmd = [\n",
    "    \"docker\", \"run\", \"-d\",\n",
    "    \"--name\", f\"fused_moe_layer_{layer}_exp_{(i+NUM_CONTAINERS)*experts_count_per_container}_{(i+NUM_CONTAINERS+1)*experts_count_per_container-1}\",\n",
    "    \"--gpus\", \"all\",\n",
    "    \"--rm\",\n",
    "    \"--rm\",\n",
    "    \"-p\", f\"{BASE_PORT+i+NUM_CONTAINERS}:5000\",\n",
    "    \"-v\", \"/home/ubuntu/vllm_test_field/vllm/ipc_handler_demo/weights:/app/weights\",\n",
    "    \"-e\", f\"RANK={i}\",\n",
    "    \"-e\", f\"NUM_EXPERTS={experts_count_per_container}\",\n",
    "    \"-e\", f\"GPU_IDX={0}\",\n",
    "    \"-e\", f\"WEIGHT_PATH=/app/weights\",\n",
    "    \"-e\", f\"LAYER={layer}\",\n",
    "    \"-e\", f\"SELF_WARMUP={True}\",\n",
    "    # \"-e\", f\"LOADED_EXPERTS={list(range(i*experts_count_per_container,(i+1)*experts_count_per_container))}\",\n",
    "\n",
    "    \n",
    "    \n",
    "    IMAGE_NAME\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"fused_moe_layer_{layer}_exp_{(i+NUM_CONTAINERS)*experts_count_per_container}_{(i+NUM_CONTAINERS+1)*experts_count_per_container-1}\\n容器启动成功！\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"启动失败: {e}\")\n",
    "        \n",
    "\n",
    "end_time = time.time()  # 记录结束时间\n",
    "elapsed_time = end_time - docker_start_time  # 计算用时\n",
    "print(f\"Containers launched in: {elapsed_time*1000:.2f} ms\")\n",
    "print(f\"Average containers launching time : {elapsed_time*1000/WORLD_SIZE:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c91f07",
   "metadata": {},
   "source": [
    "## Wait for containers to be fully initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92324a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898938c",
   "metadata": {},
   "source": [
    "## Improved LB and modify topkids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811517c4",
   "metadata": {},
   "source": [
    "## Load balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d92273f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load balance1 took 3.11 ms\n",
      "Load balance2 took 0.47 ms\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import time\n",
    "\n",
    " # 记录开始时间\n",
    "\n",
    "def select_expert(topk_ids):\n",
    "    # return expert_id, max_token_num\n",
    "    counts = Counter(topk_ids.flatten().tolist())\n",
    "    # sorted_counts_cnt = sorted(counts.items(), key=lambda x: x[1],reverse=True)\n",
    "    average =( sum(counts.values())+1) // len(counts.values())\n",
    "    \n",
    "    # print(f\"The average experts load is: {average}\")\n",
    "\n",
    "    # exceeded_loads = {k: (v - average if v > average else 0) for k, v in sorted_counts_cnt}\n",
    "    # return exceeded_loads,average\n",
    "    return average\n",
    "\n",
    "def select_expert2(topk_ids):\n",
    "    # Count expert occurrences\n",
    "    counts = torch.bincount(topk_ids.view(-1))\n",
    "    average_load = (counts.sum() + 1) // len(counts)\n",
    "    return average_load\n",
    "\n",
    "expert_load_balancing_starts = time.time()\n",
    "average_load_cpu = select_expert(inputs[\"topk_ids\"])\n",
    "# average_load = select_expert2(inputs[\"topk_ids\"])\n",
    "expert_load_balancing_ends = time.time()\n",
    "\n",
    "expert_load_balancing_elapsed_time=expert_load_balancing_ends - expert_load_balancing_starts\n",
    "\n",
    "print(f\"Load balance1 took {expert_load_balancing_elapsed_time*1000:.2f} ms\")\n",
    "\n",
    "\n",
    "expert_load_balancing_starts = time.time()\n",
    "# average_load = select_expert(inputs[\"topk_ids\"])\n",
    "average_load = select_expert2(inputs[\"topk_ids\"])\n",
    "expert_load_balancing_ends = time.time()\n",
    "\n",
    "expert_load_balancing_elapsed_time=expert_load_balancing_ends - expert_load_balancing_starts\n",
    "\n",
    "print(f\"Load balance2 took {expert_load_balancing_elapsed_time*1000:.2f} ms\")\n",
    "\n",
    "\n",
    "# print(\"exceeded_expert_load: \", sorted(exceeded_expert_load.items(), key=lambda x: x[0],reverse=False))\n",
    "# print(\"total exceeded_expert_load: \", sum(exceeded_expert_load.values()))\n",
    "# print(\"max load in exceeded_expert_load: \", max(exceeded_expert_load.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99077ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7029a",
   "metadata": {},
   "source": [
    "## modify topk ids with containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc2d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify topk ids took 8.38 ms\n",
      "Modify topk ids2 took 0.93 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def modify_topk_ids_container(topk_ids, expert_num,average_load):\n",
    "    topk_ids=topk_ids.cpu().tolist()\n",
    "\n",
    "    # 将一半 selected_id 的 workload 转移给新 expert\n",
    "    cnt = defaultdict(int)\n",
    "    \n",
    "    for i in range(len(topk_ids)):\n",
    "        for j in range(len(topk_ids[0])):\n",
    "            if cnt[topk_ids[i][j]] <= average_load:\n",
    "                cnt[topk_ids[i][j]] += 1\n",
    "            else:\n",
    "                # print(f\"{int(topk_ids[i][j])} -> {int(topk_ids[i][j]+expert_num)}\")\n",
    "                # print(f\"{int(topk_ids[i][j])} -> {int(topk_ids[i][j]+expert_num)}\")\n",
    "                topk_ids[i][j] = expert_num + topk_ids[i][j]\n",
    "                \n",
    "           \n",
    "\n",
    "    return topk_ids\n",
    "\n",
    "\n",
    "def modify_topk_ids_container2(topk_ids, expert_num, average_load):\n",
    "    # Calculate counts for each expert\n",
    "    unique_experts, counts = torch.unique(topk_ids, return_counts=True)\n",
    "    \n",
    "    # Find experts that exceed average load\n",
    "    overloaded = unique_experts[counts > average_load]\n",
    "    \n",
    "    # Create mask for overloaded experts\n",
    "    mask = torch.isin(topk_ids, overloaded)\n",
    "    \n",
    "    # Shift the overloaded expert IDs\n",
    "    topk_ids[mask] += expert_num\n",
    "    \n",
    "    return topk_ids\n",
    "\n",
    "\n",
    "topk_ids_modification_starts = time.time()\n",
    "\n",
    "newly_generated_topk_ids = modify_topk_ids_container(inputs[\"topk_ids\"], 60,average_load_cpu)\n",
    "\n",
    "topk_ids_modification_ends = time.time()\n",
    "\n",
    "topk_ids_modification_elapsed_time=topk_ids_modification_ends - topk_ids_modification_starts\n",
    "\n",
    "print(f\"Modify topk ids took {topk_ids_modification_elapsed_time*1000:.2f} ms\")\n",
    "\n",
    "\n",
    "\n",
    "topk_ids_modification_starts = time.time()\n",
    "\n",
    "newly_generated_topk_ids = modify_topk_ids_container2(inputs[\"topk_ids\"], 60,average_load)\n",
    "\n",
    "topk_ids_modification_ends = time.time()\n",
    "\n",
    "topk_ids_modification_elapsed_time=topk_ids_modification_ends - topk_ids_modification_starts\n",
    "\n",
    "print(f\"Modify topk ids2 took {topk_ids_modification_elapsed_time*1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd1707",
   "metadata": {},
   "source": [
    "## Initialize expert map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c695d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "container expert maps\n",
      "\n",
      "\n",
      "Expert map initialization took 21.57 ms\n"
      "Expert map initialization took 21.57 ms\n"
     ]
    }
   ],
   "source": [
    "def init_expert_map(rank, world_size, total_expert_num):\n",
    "    avg_expert_num = total_expert_num // world_size\n",
    "    real_local_expert_ids = list(range(rank * avg_expert_num, (rank + 1) * avg_expert_num))\n",
    "    expert_map = [-1] * total_expert_num\n",
    "    for local_id, global_id in enumerate(real_local_expert_ids):\n",
    "        expert_map[global_id] = local_id\n",
    "    return torch.tensor(expert_map, device=\"cuda\", dtype=torch.int32)\n",
    "\n",
    "container_expert_maps=[]\n",
    "print(\"\\ncontainer expert maps\\n\\n\")\n",
    "\n",
    "expert_map_initialize_starts = time.time()\n",
    "\n",
    "\n",
    "expert_map_initialize_starts = time.time()\n",
    "\n",
    "\n",
    "for worker in workers:\n",
    "    worker[\"expert_map\"]  = init_expert_map( worker[\"rank\"],WORLD_SIZE,  config.num_experts)\n",
    "    minus_ones = torch.full((60,), -1).cuda()\n",
    "    worker[\"expert_map\"]  = torch.cat((worker[\"expert_map\"] ,minus_ones))\n",
    "    # print(worker[\"expert_map\"], worker[\"expert_map\"].shape)\n",
    "    # print(worker[\"expert_map\"], worker[\"expert_map\"].shape)\n",
    "    container_expert_maps.append(worker[\"expert_map\"].cpu().tolist())\n",
    "\n",
    "for worker in workers:\n",
    "    em  = init_expert_map( worker[\"rank\"],WORLD_SIZE,  config.num_experts)\n",
    "    # 创建包含60个-1的Tensor\n",
    "    minus_ones = torch.full((60,), -1).cuda()\n",
    "    em  = torch.cat((minus_ones,em))\n",
    "    # print(em, em.shape)\n",
    "    # print(em, em.shape)\n",
    "    container_expert_maps.append(em.cpu().tolist())\n",
    "    \n",
    "expert_map_initialize_ends = time.time()\n",
    "expert_map_initialize_elapsed_time=expert_map_initialize_ends - expert_map_initialize_starts\n",
    "print(f\"Expert map initialization took {expert_map_initialize_elapsed_time*1000:.2f} ms\")\n",
    "    \n",
    "expert_map_initialize_ends = time.time()\n",
    "expert_map_initialize_elapsed_time=expert_map_initialize_ends - expert_map_initialize_starts\n",
    "print(f\"Expert map initialization took {expert_map_initialize_elapsed_time*1000:.2f} ms\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82a17d",
   "metadata": {},
   "source": [
    "## Pure using container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93630db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU to CPU took 81.84 ms\n"
     ]
    }
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU to CPU took 81.84 ms\n"
     ]
    }
   ],
   "source": [
    "gpu2cpu_starts = time.time()\n",
    "\n",
    "\n",
    "gpu2cpu_starts = time.time()\n",
    "\n",
    "\n",
    "inputs[\"topk_ids\"] = inputs[\"topk_ids\"].cpu().tolist()\n",
    "inputs[\"hidden_states\"] = inputs[\"hidden_states\"].cpu().tolist()\n",
    "inputs[\"topk_weights\"] = inputs[\"topk_weights\"].cpu().tolist()\n",
    "gpu2cpu_ends = time.time()\n",
    "\n",
    "gpu2cpu_elapsed_time = gpu2cpu_ends - gpu2cpu_starts\n",
    "\n",
    "print(f\"GPU to CPU took {gpu2cpu_elapsed_time*1000:.2f} ms\")"
    "inputs[\"topk_weights\"] = inputs[\"topk_weights\"].cpu().tolist()\n",
    "gpu2cpu_ends = time.time()\n",
    "\n",
    "gpu2cpu_elapsed_time = gpu2cpu_ends - gpu2cpu_starts\n",
    "\n",
    "print(f\"GPU to CPU took {gpu2cpu_elapsed_time*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de292f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(container_expert_maps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9cb03cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per request time is: 2551.05 ms\n",
      "Container 0 latency: 1.560256004333496 ms\n",
      "per request time is: 2551.05 ms\n",
      "Container 0 latency: 1.560256004333496 ms\n",
      "output.shape torch.Size([1000, 2048])\n",
      "per request time is: 2531.03 ms\n",
      "Container 1 latency: 1.5052800178527832 ms\n",
      "per request time is: 2531.03 ms\n",
      "Container 1 latency: 1.5052800178527832 ms\n",
      "output.shape torch.Size([1000, 2048])\n",
      "per request time is: 2528.07 ms\n",
      "Container 2 latency: 1.4544639587402344 ms\n",
      "per request time is: 2528.07 ms\n",
      "Container 2 latency: 1.4544639587402344 ms\n",
      "output.shape torch.Size([1000, 2048])\n",
      "per request time is: 2526.27 ms\n",
      "Container 3 latency: 1.4755840301513672 ms\n",
      "per request time is: 2526.27 ms\n",
      "Container 3 latency: 1.4755840301513672 ms\n",
      "output.shape torch.Size([1000, 2048])\n",
      "per request time is: 2029.37 ms\n",
      "Container 4 latency: 0.8275200128555298 ms\n",
      "per request time is: 2029.37 ms\n",
      "Container 4 latency: 0.8275200128555298 ms\n",
      "output.shape torch.Size([1000, 2048])\n",
      "per request time is: 2017.89 ms\n",
      "Container 5 latency: 0.8238400220870972 ms\n",
      "per request time is: 2017.89 ms\n",
      "Container 5 latency: 0.8238400220870972 ms\n",
      "output.shape torch.Size([1000, 2048])\n",
      "per request time is: 2028.30 ms\n",
      "Container 6 latency: 0.8133440017700195 ms\n",
      "per request time is: 2028.30 ms\n",
      "Container 6 latency: 0.8133440017700195 ms\n",
      "output.shape torch.Size([1000, 2048])\n",
      "per request time is: 1990.39 ms\n",
      "Container 7 latency: 0.8216000199317932 ms\n",
      "per request time is: 1990.39 ms\n",
      "Container 7 latency: 0.8216000199317932 ms\n",
      "output.shape torch.Size([1000, 2048])\n",
      "End to End time is: 20452.31 ms\n",
      "End to End time(ideal) is: 117.59 ms\n"
      "End to End time is: 20452.31 ms\n",
      "End to End time(ideal) is: 117.59 ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "out_hidden_states = [None]*WORLD_SIZE*2\n",
    "start_time = time.time() \n",
    "\n",
    "\n",
    "total_time = 0\n",
    "start_time = time.time() \n",
    "\n",
    "\n",
    "total_time = 0\n",
    "for i in range(WORLD_SIZE*2):\n",
    "    url = f\"http://localhost:500{i}/forward\"\n",
    "    inputs[\"expert_map\"] = container_expert_maps[i]\n",
    "    start_req_time = time.time()\n",
    "    start_req_time = time.time()\n",
    "    response = requests.post(url, json=inputs)\n",
    "    output = torch.tensor(response.json()[\"hidden_output\"],dtype=torch.bfloat16,device=\"cuda:0\")\n",
    "    end_req_time = time.time()\n",
    "    print(f\"per request time is: {(end_req_time-start_req_time)*1000:.2f} ms\")\n",
    "    end_req_time = time.time()\n",
    "    print(f\"per request time is: {(end_req_time-start_req_time)*1000:.2f} ms\")\n",
    "    \n",
    "    latency_ms = response.json()[\"latency_ms\"]\n",
    "    total_time += latency_ms/1000\n",
    "    total_time += latency_ms/1000\n",
    "    print(f\"Container {i} latency: {latency_ms} ms\")\n",
    "    print(\"output.shape\", output.shape)\n",
    "    out_hidden_states.append(output)    \n",
    "\n",
    "    out_hidden_states.append(output)    \n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()  # 记录结束时间\n",
    "elapsed_time = end_time - start_time  # 计算耗时\n",
    "\n",
    "total_time += gpu2cpu_elapsed_time + expert_map_initialize_elapsed_time + topk_ids_modification_elapsed_time + expert_load_balancing_elapsed_time\n",
    "print(f\"End to End time is: {elapsed_time*1000:.2f} ms\")\n",
    "print(f\"End to End time(ideal) is: {total_time*1000:.2f} ms\")"
    "total_time += gpu2cpu_elapsed_time + expert_map_initialize_elapsed_time + topk_ids_modification_elapsed_time + expert_load_balancing_elapsed_time\n",
    "print(f\"End to End time is: {elapsed_time*1000:.2f} ms\")\n",
    "print(f\"End to End time(ideal) is: {total_time*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd1768f2",
   "execution_count": 14,
   "id": "cd1768f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08184170722961426,\n",
       " 0.021572589874267578,\n",
       " 0.0027360916137695312,\n",
       " 0.0021560192108154297)"
       "(0.08184170722961426,\n",
       " 0.021572589874267578,\n",
       " 0.0027360916137695312,\n",
       " 0.0021560192108154297)"
      ]
     },
     "execution_count": 14,
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gpu2cpu_elapsed_time , expert_map_initialize_elapsed_time , topk_ids_modification_elapsed_time , expert_load_balancing_elapsed_time)"
    "(gpu2cpu_elapsed_time , expert_map_initialize_elapsed_time , topk_ids_modification_elapsed_time , expert_load_balancing_elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
