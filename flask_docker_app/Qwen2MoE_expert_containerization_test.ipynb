{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51d0223",
   "metadata": {},
   "source": [
    "## Container Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017758fb",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef62967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "import requests\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "\n",
    "client = docker.from_env()\n",
    "\n",
    "# --- Configs ---\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "NUM_CONTAINERS = 2\n",
    "IMAGE_NAME = \"moe_expert\"\n",
    "EXPERT_TIMEOUT = 20  # seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213789a",
   "metadata": {},
   "source": [
    "### Build Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0946db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRECATED: The legacy builder is deprecated and will be removed in a future release.\n",
      "            Install the buildx component to build images with BuildKit:\n",
      "            https://docs.docker.com/go/buildx/\n",
      "\n",
      "Sending build context to Docker daemon  303.1kB\n",
      "Step 1/7 : FROM nvidia/cuda:12.1.1-base-ubuntu22.04\n",
      " ---> 72d1c5868625\n",
      "Step 2/7 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> a83412ad1a8a\n",
      "Step 3/7 : RUN apt-get update && apt-get install -y git python3 python3-pip python3-dev && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 024bf909d6ed\n",
      "Step 4/7 : RUN pip3 install -v flask\n",
      " ---> Using cache\n",
      " ---> 6101706050d4\n",
      "Step 5/7 : RUN pip3 install -v vllm\n",
      " ---> Using cache\n",
      " ---> b8e07ac7f122\n",
      "Step 6/7 : COPY expert.py /app\n",
      " ---> 8b09389f2fb1\n",
      "Step 7/7 : CMD [\"python3\", \"./expert.py\"]\n",
      " ---> Running in de972e1ed3d5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---> Removed intermediate container de972e1ed3d5\n",
      " ---> 26ebb4b64c05\n",
      "Successfully built 26ebb4b64c05\n",
      "Successfully tagged moe_expert:latest\n"
     ]
    }
   ],
   "source": [
    "! docker build -t moe_expert ~/vllm_test_field/vllm/flask_docker_app/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c80100",
   "metadata": {},
   "source": [
    "## Launching instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea405bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09770c2cbc63eb1f735b9711de6d757dd1048c3e4d4674e656feef1f7d3fb76d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fused_moe_layer_23_exp_0_29\n",
      "容器启动成功！\n",
      "76350462c9e7c744d957a38dd3a53f73fe4465405af9c94393916327dfb4f5ea\n",
      "fused_moe_layer_23_exp_30_59\n",
      "容器启动成功！\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# --- Step 2: Launch Expert Containers ---\n",
    "containers = []\n",
    "layer = 23\n",
    "\n",
    "experts_count_per_container = 60//NUM_CONTAINERS\n",
    "for i in range(NUM_CONTAINERS):\n",
    "    cmd = [\n",
    "    \"docker\", \"run\", \"-d\",\n",
    "    \"--name\", f\"fused_moe_layer_{layer}_exp_{i*experts_count_per_container}_{(i+1)*experts_count_per_container-1}\",\n",
    "    \"--gpus\", \"all\",\n",
    "    \"--rm\",\n",
    "    \"-p\", f\"{5000+i}:5000\",\n",
    "    \"-v\", \"/home/ubuntu/vllm_test_field/vllm/ipc_handler_demo/weights:/app/weights\",\n",
    "    \"-e\", f\"RANK={i}\",\n",
    "    \"-e\", f\"NUM_EXPERTS={experts_count_per_container}\",\n",
    "    \"-e\", f\"GPU_IDX={0}\",\n",
    "    \"-e\", f\"WEIGHT_PATH=/app/weights\",\n",
    "    \"-e\", f\"LAYER={layer}\",\n",
    "    \n",
    "    IMAGE_NAME\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"fused_moe_layer_{layer}_exp_{i*experts_count_per_container}_{(i+1)*experts_count_per_container-1}\\n容器启动成功！\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"启动失败: {e}\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3eb9a9",
   "metadata": {},
   "source": [
    "### View if the instance is running correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24d3aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE        COMMAND                 CREATED        STATUS                  PORTS                                       NAMES\n",
      "c3ae387a3bd9   moe_expert   \"python3 ./expert.py\"   1 second ago   Up Less than a second   0.0.0.0:5001->5000/tcp, :::5001->5000/tcp   fused_moe_layer_23_exp_30_59\n",
      "5a27ecb9280a   moe_expert   \"python3 ./expert.py\"   1 second ago   Up Less than a second   0.0.0.0:5000->5000/tcp, :::5000->5000/tcp   fused_moe_layer_23_exp_0_29\n"
     ]
    }
   ],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8acb0",
   "metadata": {},
   "source": [
    "## Testing container function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d359d29",
   "metadata": {},
   "source": [
    "### Load Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affb7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "def load_tensor(path):\n",
    "    tensors = {}\n",
    "    try:\n",
    "        tensors[\"hidden_states\"] = torch.load(path + \"hidden_states.pt\")\n",
    "        tensors[\"w1\"] = torch.load(path + \"w1.pt\")\n",
    "        tensors[\"w2\"] = torch.load(path + \"w2.pt\")\n",
    "        tensors[\"topk_weights\"] = torch.load(path + \"topk_weights.pt\")\n",
    "        tensors[\"topk_ids\"] = torch.load(path + \"topk_ids.pt\")\n",
    "        tensors[\"expert_map\"] = torch.load(path + \"expert_map.pt\")\n",
    "        tensors[\"out_hidden_states\"] = torch.load(path + \"out_hidden_states.pt\")\n",
    "        tensors[\"final_hidden_states\"] = torch.load(path + \"final_hidden_states.pt\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error: CUDA runtime issue - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    return tensors\n",
    "\n",
    "rank0 = load_tensor(\"/home/ubuntu/vllm_test_field/saved_tensors/rank_0/\")\n",
    "rank1 = load_tensor(\"/home/ubuntu/vllm_test_field/saved_tensors/rank_1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce610d2",
   "metadata": {},
   "source": [
    "### Prepare handler and meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ctypes\n",
    "\n",
    "# Load the shared lib\n",
    "lib = ctypes.CDLL('./cuda_tools/libipc_tensor_tool.so')\n",
    "lib.export_ipc_handle.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n",
    "lib.export_ipc_handle.restype = ctypes.c_int\n",
    "\n",
    "def get_ipc_handle(tensor: torch.Tensor) -> bytes:\n",
    "    \n",
    "    meta={\n",
    "        \"shape\": tensor.shape,\n",
    "        \"dtype\": str(tensor.dtype),\n",
    "        \"device\": int(tensor.device.index),\n",
    "    }\n",
    "    if not tensor.is_cuda:\n",
    "        raise ValueError(\"Tensor must be on CUDA device\")\n",
    "\n",
    "    dev_ptr = tensor.data_ptr()\n",
    "    out = ctypes.create_string_buffer(64)\n",
    "\n",
    "    result = lib.export_ipc_handle(ctypes.c_void_p(dev_ptr), out)\n",
    "    if result != 0:\n",
    "        raise RuntimeError(f\"export_ipc_handle failed with code {result}\")\n",
    "\n",
    "    return out.raw, meta  # This is the 64-byte IPC handle\n",
    "\n",
    "hidden_states_handler, hidden_states_meta = get_ipc_handle(rank0[\"hidden_states\"])\n",
    "topk_weights_handler, topk_weights_meta = get_ipc_handle(rank0[\"topk_weights\"])\n",
    "topk_ids_handler, topk_ids_meta = get_ipc_handle(rank0[\"topk_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834810fd",
   "metadata": {},
   "source": [
    "### Sending hidden states to instances for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c6d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "url1 = \"http://localhost:5000/forward\"\n",
    "# url2 = \"http://localhost:5001/forward\"\n",
    "\n",
    "response1 = requests.post(url1, \n",
    "                          data={\n",
    "        'hidden_states_meta': json.dumps(hidden_states_meta),\n",
    "        'topk_weights_meta': json.dumps(topk_weights_meta),\n",
    "        'topk_ids_meta': json.dumps(topk_ids_meta),\n",
    "},\n",
    "files={\n",
    "        'hidden_states_handler': ('data.bin', hidden_states_handler, 'application/octet-stream'),\n",
    "        'topk_weights_handler': ('data.bin', topk_weights_handler, 'application/octet-stream'),\n",
    "        'topk_ids_handler': ('data.bin', topk_ids_handler, 'application/octet-stream'),\n",
    "})\n",
    "\n",
    "# response2 = requests.post(url2, json={\n",
    "#         \"hidden_states\":rank1[\"hidden_states\"].cpu().tolist(),\n",
    "#         \"topk_weights\": rank1[\"topk_weights\"].cpu().tolist(),\n",
    "#         \"topk_ids\": rank1[\"topk_ids\"].cpu().tolist()\n",
    "#         })\n",
    "\n",
    "output1 = torch.tensor(response1.json()[\"hidden_output\"],dtype=torch.bfloat16,device=\"cuda:0\")\n",
    "# output2 = torch.tensor(response2.json()[\"hidden_output\"],dtype=torch.bfloat16,device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1203b270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print( torch.equal(output1 , rank0[\"out_hidden_states\"]) )\n",
    "print( torch.equal(output2 , rank1[\"out_hidden_states\"].cuda()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa310b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "rank0[\"shared_output\"] = torch.load(\"/home/ubuntu/vllm_test_field/saved_tensors/rank_0/shared_output.pt\")\n",
    "rank1[\"shared_output\"] = torch.load(\"/home/ubuntu/vllm_test_field/saved_tensors/rank_1/shared_output.pt\")\n",
    "rank0_final_hidden_states = output1 + rank0[\"shared_output\"]\n",
    "rank1_final_hidden_states = output2 + rank1[\"shared_output\"].cuda()\n",
    "# all reduce \n",
    "reduced_result = rank0_final_hidden_states + rank1_final_hidden_states.to(rank0_final_hidden_states.device)\n",
    "print( torch.equal(reduced_result, \n",
    "                   rank0[\"final_hidden_states\"]) )\n",
    "\n",
    "print( torch.equal(reduced_result, \n",
    "                   rank1[\"final_hidden_states\"].cuda()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89fb7f",
   "metadata": {},
   "source": [
    "## Turn off the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342bab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76350462c9e7\n",
      "09770c2cbc63\n",
      "已停止 2 个容器: ['76350462c9e7', '09770c2cbc63']\n"
     ]
    }
   ],
   "source": [
    "def stop_fused_moe_containers():\n",
    "    try:\n",
    "        # 1. 获取所有名称包含 'fused_moe_layer' 的容器ID\n",
    "        grep_cmd = \"docker ps --filter 'name=fused_moe_layer' -q\"\n",
    "        container_ids = subprocess.check_output(grep_cmd, shell=True, text=True).strip().split('\\n')\n",
    "        \n",
    "        # 2. 批量停止容器\n",
    "        if container_ids and container_ids[0]:  # 如果有匹配的容器\n",
    "            stop_cmd = f\"docker stop {' '.join(container_ids)}\"\n",
    "            subprocess.run(stop_cmd, shell=True, check=True)\n",
    "            print(f\"已停止 {len(container_ids)} 个容器: {container_ids}\")\n",
    "        else:\n",
    "            print(\"没有找到名称包含 'fused_moe_layer' 的容器\")\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"操作失败: {e}\")\n",
    "\n",
    "# 执行函数\n",
    "stop_fused_moe_containers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
